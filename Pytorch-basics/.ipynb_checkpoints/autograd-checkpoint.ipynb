{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "\n",
    "自动求导是Pytorch中非常重要的特性，能够让我们避免手动去计算非常复杂的导数，这能够极大地减少我们构建模型的时间，这也是其前身Torch这个框架所不具备的特性，下面通过例子学习一下Pytorch自动求导的独特魅力以及探究自动求导的更多用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单情况下的自动求导\n",
    "\n",
    "下面我们显示一些简单情况的自动求导，”简单“体现在计算的结果都是标量，也就是一个数，我们对这个标量进行自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 19\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "y = x + 2\n",
    "z = y ** 2 + 3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的一些列的操作，我们从x得到了最后的结果out，可以将其表示为数学公式\n",
    "![](https://render.githubusercontent.com/render/math?math=z%20%3D%20%28x%20%2B%202%29%5E2%20%2B%203&mode=display)\n",
    "\n",
    "那么我们从z到x求导的结果就是\n",
    "![](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20z%7D%7B%5Cpartial%20x%7D%20%3D%202%20%28x%20%2B%202%29%20%3D%202%20%282%20%2B%202%29%20%3D%208&mode=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 8\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用自动求导\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上面这样的一个例子，我们验证了自动求导，同时可以发现使用自动求导非常方便。如果是一个更加复杂的例子，那么手动求导就会显得非常的麻烦，所以自动求导的机制能够帮助省去麻烦的数学计算，下面我们可以看一个更加复杂的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 20), requires_grad=True)\n",
    "y = Variable(torch.randn(10,5), requires_grad=True)\n",
    "w = Variable(torch.randn(20, 5), requires_grad=True)\n",
    "\n",
    "out = torch.mean(y-torch.matmul(x,w))\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      " -1.9080  3.5082 -1.4794  4.0858 -5.1826  0.1013 -0.9483 -0.2599 -6.0009 -2.8681\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "  6.4171  1.1695  0.6390  0.4664 -2.7299 -5.8308 -2.4000  7.5692 -9.4507 -0.2978\n",
      "[torch.FloatTensor of size 10x20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "  2.0000  2.0000  2.0000  2.0000  2.0000\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y的梯度\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0287 -0.0287 -0.0287 -0.0287 -0.0287\n",
      "-0.0643 -0.0643 -0.0643 -0.0643 -0.0643\n",
      " 0.0368  0.0368  0.0368  0.0368  0.0368\n",
      " 0.0029  0.0029  0.0029  0.0029  0.0029\n",
      "-0.0356 -0.0356 -0.0356 -0.0356 -0.0356\n",
      "-0.0724 -0.0724 -0.0724 -0.0724 -0.0724\n",
      "-0.0092 -0.0092 -0.0092 -0.0092 -0.0092\n",
      "-0.0136 -0.0136 -0.0136 -0.0136 -0.0136\n",
      "-0.0689 -0.0689 -0.0689 -0.0689 -0.0689\n",
      " 0.0154  0.0154  0.0154  0.0154  0.0154\n",
      " 0.0362  0.0362  0.0362  0.0362  0.0362\n",
      "-0.0143 -0.0143 -0.0143 -0.0143 -0.0143\n",
      "-0.0404 -0.0404 -0.0404 -0.0404 -0.0404\n",
      " 0.1268  0.1268  0.1268  0.1268  0.1268\n",
      " 0.0384  0.0384  0.0384  0.0384  0.0384\n",
      " 0.0063  0.0063  0.0063  0.0063  0.0063\n",
      "-0.0273 -0.0273 -0.0273 -0.0273 -0.0273\n",
      " 0.0081  0.0081  0.0081  0.0081  0.0081\n",
      "-0.0254 -0.0254 -0.0254 -0.0254 -0.0254\n",
      "-0.0162 -0.0162 -0.0162 -0.0162 -0.0162\n",
      "[torch.FloatTensor of size 20x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# w的梯度\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面数学公式更加的复杂，矩阵乘法之后对两个矩阵对应元素相乘，然后所有元素求平均，手动计算梯度太麻烦了，我是不想算了....\n",
    "使用Pytorch自动求导，能够非常容易得到x,y和w的导数，因为深度学习中充满大量的矩阵运算，所以我们没有办法对求这些导数，有了自动求导能够非常方便解决网络更新的问题。  \n",
    "\n",
    "## 复杂情况的自动求导\n",
    "上面我们展示了简单情况下的自动求导，都是对标量进行自动求导，可能会有一个疑问，如何对一个向量或者矩阵自动求导呢  \n",
    "下面介绍对多维数组的自动求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  3\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = Variable(torch.FloatTensor([[2,3]]), requires_grad=True) # 构建一个1x2的矩阵\n",
    "n = Variable(torch.zeros(1,2)) # 构建一个相同大小的0矩阵\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  4  27\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过m中的值计算新的n中的值\n",
    "n[0, 0] = m[0, 0] ** 2\n",
    "n[0, 1] = m[0, 1] ** 3\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面的式子写成数学公式，可以得到  \n",
    "![](https://render.githubusercontent.com/render/math?math=n%20%3D%20%28n_0%2C%5C%20n_1%29%20%3D%20%28m_0%5E2%2C%5C%20m_1%5E3%29%20%3D%20%282%5E2%2C%5C%203%5E3%29&mode=display)\n",
    "下面我们直接对n进行反向传播，也就是求n对m的导数。  \n",
    "这时我们需要明确这个导数的定义，即如何定义\n",
    "![](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20n%7D%7B%5Cpartial%20m%7D%20%3D%20%5Cfrac%7B%5Cpartial%20%28n_0%2C%5C%20n_1%29%7D%7B%5Cpartial%20%28m_0%2C%5C%20m_1%29%7D&mode=display)\n",
    "在PyTorch中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和n一样大，比如是![](https://render.githubusercontent.com/render/math?math=%28w_0%2C%5C%20w_1%29&mode=inline)，那么自动求导就是：\n",
    "![](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20n%7D%7B%5Cpartial%20m_1%7D%20%3D%20w_0%20%5Cfrac%7B%5Cpartial%20n_0%7D%7B%5Cpartial%20m_1%7D%20%2B%20w_1%20%5Cfrac%7B%5Cpartial%20n_1%7D%7B%5Cpartial%20m_1%7D&mode=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  4  27\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n.backward(torch.ones_like(n))\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过自动求导得到了梯度是4 和 27，验算一下![](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20n%7D%7B%5Cpartial%20m_0%7D%20%3D%20w_0%20%5Cfrac%7B%5Cpartial%20n_0%7D%7B%5Cpartial%20m_0%7D%20%2B%20w_1%20%5Cfrac%7B%5Cpartial%20n_1%7D%7B%5Cpartial%20m_0%7D%20%3D%202%20m_0%20%2B%200%20%3D%202%20%5Ctimes%202%20%3D%204&mode=display)![](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20n%7D%7B%5Cpartial%20m_1%7D%20%3D%20w_0%20%5Cfrac%7B%5Cpartial%20n_0%7D%7B%5Cpartial%20m_1%7D%20%2B%20w_1%20%5Cfrac%7B%5Cpartial%20n_1%7D%7B%5Cpartial%20m_1%7D%20%3D%200%20%2B%203%20m_1%5E2%20%3D%203%20%5Ctimes%203%5E2%20%3D%2027&mode=display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多次自动求导\n",
    "通过调用backward可以进行一次自动求导，如果再次调用backward，则会报错。因为PyTorch默认做完一次自动求导后，计算图就被丢弃了，所以两次自动求导需要手动设置一个参数，通过下面例子说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
    "y = x * 2 + x ** 2 + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 retain_graph 为 True 来保留计算图\n",
    "y.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 8\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 16\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现x的梯度变成了16， 因为这里做了两次自动求导，所以将第一次的梯度8和第二次的梯度8加起来就是16\n",
    "\n",
    "### 小练习\n",
    "定义![](https://render.githubusercontent.com/render/math?math=x%20%3D%20%0A%5Cleft%5B%0A%5Cbegin%7Bmatrix%7D%0Ax_0%20%5C%5C%0Ax_1%0A%5Cend%7Bmatrix%7D%0A%5Cright%5D%20%3D%20%0A%5Cleft%5B%0A%5Cbegin%7Bmatrix%7D%0A2%20%5C%5C%0A3%0A%5Cend%7Bmatrix%7D%0A%5Cright%5D%0A%24%24%24%24%0Ak%20%3D%20%28k_0%2C%5C%20k_1%29%20%3D%20%28x_0%5E2%20%2B%203%20x_1%2C%5C%202%20x_0%20%2B%20x_1%5E2%29&mode=display)\n",
    "求![](https://render.githubusercontent.com/render/math?math=j%20%3D%20%5Cleft%5B%0A%5Cbegin%7Bmatrix%7D%0A%5Cfrac%7B%5Cpartial%20k_0%7D%7B%5Cpartial%20x_0%7D%20%26amp%3B%20%5Cfrac%7B%5Cpartial%20k_0%7D%7B%5Cpartial%20x_1%7D%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20k_1%7D%7B%5Cpartial%20x_0%7D%20%26amp%3B%20%5Cfrac%7B%5Cpartial%20k_1%7D%7B%5Cpartial%20x_1%7D%0A%5Cend%7Bmatrix%7D%0A%5Cright%5D&mode=display)\n",
    "结果：\n",
    "![](https://render.githubusercontent.com/render/math?math=%5Cleft%5B%0A%5Cbegin%7Bmatrix%7D%0A4%20%26amp%3B%203%20%5C%5C%0A2%20%26amp%3B%206%20%5C%5C%0A%5Cend%7Bmatrix%7D%0A%5Cright%5D&mode=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.FloatTensor([2, 3]), requires_grad=True)\n",
    "k = Variable(torch.zeros(2))\n",
    "\n",
    "k[0] = x[0]**2 + x[1]*3\n",
    "k[1] = 2*x[0] + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 13\n",
      " 13\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = torch.zeros(2, 2)\n",
    "\n",
    "k.backward(torch.FloatTensor([1, 0]), retain_graph=True)\n",
    "j[0] = x.grad.data\n",
    "\n",
    "x.grad.data.zero_() # 归零之前求得的梯度\n",
    "\n",
    "k.backward(torch.FloatTensor([0, 1]))\n",
    "j[1] = x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4  3\n",
      " 2  6\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
